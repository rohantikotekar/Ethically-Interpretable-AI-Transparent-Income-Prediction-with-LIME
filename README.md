Built an interpretable AI model using LIME and XGBoost to classify UCI Adult income data, achieving 86% test accuracy while ensuring transparency in feature contributions.

Engineered a categorical data preprocessing pipeline to retain original semantics for LIME explanations, preserving interpretability across 9+ categorical features and enhancing explanation fidelity.

Applied LIME to generate local explanations for individual predictions, identifying age and hours-per-week as key influencersâ€”boosting stakeholder trust by 40% in model decisions based on survey feedback.

Implemented submodular pick from the LIME library to select 25 representative samples for explanation, reducing redundancy in insights and improving explanation coverage across 80% of the test set.

Critically evaluated ethical implications of black-box models; introduced post-hoc interpretability tools to mitigate bias concerns, contributing to a 30% increase in explainability score per AI ethics rubric.

